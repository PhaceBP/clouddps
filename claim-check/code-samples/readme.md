# Claim and Check Pattern

Code samples as discussed in the "Claim and Check" Pattern and Practices document.

## Samples

### [Sample 1: Automatic Tag Generation, Queues as Message Bus](sample-1)

In this example we're using Blob Store to store data, but any service that supportes Event Grid integration can be used too. A client just needs to drop the payload to be shared into the designated Azure Blob Store and Event Grid will automatically generate a Claim Check message and send it to one of the supported message bus. In this sample the message bus is created using Azure Storage Queues. This allows a client application to poll the queue, get the message and then use the stored reference data to download the payload directly from Azure Blob Storage.
The same message, without the need to go through a message bus, can also be directly consumed via Azure Function, leveraging in this case the serverless nature of both Azure Event Grid and Azure Functions.


### [Sample 2: Automatic Tag Generation, Event Hubs as Message Bus](sample-2)

Very similarly to the Sample 1, a reference message is automatically generated by Event Grid as soon as a payload is dropped in the designated Azure Blob Storage. Here the message bus is implemented via Event Hub, so that a client can register itself to be notified each time there is a message in the bus.
Event Hub is also configured so that it automatically archives received messages that are then available as an Avro file which is easily queryable using tools like Apache Spark, Apache Drill or any of the Avro libraries available.


### [Sample 3: Plugin support, Service Bus as Message Bus](sample-3)

This sample takes advantage of the `ServiceBus.AttachmentPlugin` which brings the claim check pattern implementation to Service Bus. The plugin is used to convert any message body into an attachment which gets stored in Azure Blob Storage on message send. Internally, service bus message is used to act as a notification queue which can subscribed on to read the message. On message receive, the plugin makes it possible to directly read the message data from blob storage in the consumer. You can then chose how you want to process the message further. The good thing about this approach is, it obscures the actual claim check workflow from the end user.

### [Sample 4: Manual Tagging](sample-4)

The reason this example uses Event Hubs with Kafka is to demonstrate the ease of using other Azure services like Azure Blob Storage, Azure functions etc. with a different messaging protocol like Kafka from your existing Kafka clients to implement the claim check messaging pattern.
This sample consists of a Kafka client which drops the payload in the designated Azure Blob Storage and creates a notification message with location details to be sent to the consumer. The notification message is sent using Event Hubs with Kafka enabled.  
The consumer is notified each time these is a message in the Event Hub and can access the payload using the location information in the message received.